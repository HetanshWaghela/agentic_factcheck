{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "05632f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "serper_api_key = os.getenv(\"SERPER_API_KEY\")\n",
    "os.environ[\"USER_AGENT\"] = \"factcheck/1.0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac38c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d2f35649",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacies_df= pd.read_csv(\"fallacies.csv\")\n",
    "fallacies_list = \"\\n\".join(\n",
    "    f\"{row['fine_class']}: {row['definition']}\" for _, row in fallacies_df.iterrows()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6c2ea8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the model\n",
    "llm = ChatOpenAI(model=\"llama3-70b-8192\", temperature=0, api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\", max_completion_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "19ef2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = \"\"\"\n",
    "You are a neutral fact-checking communications analyst. Your tasks are:\n",
    "(1) Summarize the article in five clear sentences (neutral, specific, no hype).\n",
    "(2) Extract and verify check-worthy claims.\n",
    "(3) Flag logical fallacies from the provided list.\n",
    "\n",
    "Rules:\n",
    "- Do not speculate or make things up.\n",
    "- Cite reputable, independent sources with short quotes and URLs.\n",
    "- If evidence is insufficient or conflicting, mark the claim as \"Unverifiable\" or \"Needs context\".\n",
    "- Use the provided fallacies list only; if none apply, say \"None found\".\n",
    "- Show only final answers. No hidden reasoning.\n",
    "\n",
    "Article to analyze:\n",
    "{content}\n",
    "\n",
    "Fallacy reference list:\n",
    "{fallacies_list}\n",
    "\n",
    "## Tasks\n",
    "1. **Summary**: Five-sentence neutral summary.\n",
    "2. **Claim extraction**: List 5‚Äì10 key factual claims.\n",
    "3. **Verification**: For each claim, give a verdict: True | False | Misleading | Needs context | Unverifiable.\n",
    "   - Provide 1‚Äì3 short supporting/contradicting quotes with URLs and source dates.\n",
    "   - Note if event dates match or conflict.\n",
    "4. **Fallacies**: List matching fallacies (by name) for each claim.\n",
    "5. **Red flags**: Note sensational language, anonymous sourcing, or inconsistencies (if any).\n",
    "6. **Confidence**: Provide an overall confidence score between 0 and 1.\n",
    "\n",
    "IMPORTANT: Respond ONLY with valid JSON. No additional text before or after. Use this exact structure:\n",
    "\n",
    "{{\n",
    "  \"summary\": \"Five sentences summary here...\",\n",
    "  \"claims\": [\n",
    "    {{\n",
    "      \"claim\": \"Specific claim text\",\n",
    "      \"verdict\": \"True|False|Misleading|Needs context|Unverifiable\",\n",
    "      \"evidence\": [\n",
    "        {{\"quote\": \"Short quote\", \"url\": \"https://example.com\", \"source\": \"Source name\", \"published_date\": \"YYYY-MM-DD\", \"matches_event_date\": true}}\n",
    "      ],\n",
    "      \"fallacies\": [\"Fallacy name or None found\"],\n",
    "      \"notes\": \"Brief notes if needed\"\n",
    "    }}\n",
    "  ],\n",
    "  \"red_flags\": [\"Flag 1\", \"Flag 2\"],\n",
    "  \"confidence\": 0.8\n",
    "}}\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "28cd5a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(\n",
    "        template=template1,\n",
    "        input_variables=[\"content\", \"fallacies_list\"] \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ea6e0c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "template2 = \"\"\"You are an ethics professor reviewing a news article SUMMARY. Be succinct and easy to read, but ground your critique in core ethics principles (fairness, non-maleficence, duty of care, transparency). Use ONLY the fallacy names/definitions provided below. If no fallacy applies, say \"None found\" and explain why.\n",
    "\n",
    "Article summary: {summary}\n",
    "\n",
    "Fallacies to consider:\n",
    "{fallacies_list}\n",
    "\n",
    "Provide EXACTLY:\n",
    "1) Most impactful fallacy: <name from list or \"None found\">\n",
    "2) Why this could mislead readers: <1‚Äì3 sentences, plain language>\n",
    "3) Counterfactual/counterpoint: <one plausible alternative interpretation for why this fallacy (or appearance of it) might be present>\n",
    "\n",
    "Constraints:\n",
    "- Do not invent facts beyond the summary and fallacy list.\n",
    "- No step-by-step reasoning; show final answers only.\n",
    "- Keep the total response under 120 words.\n",
    "\n",
    "Professor:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "698427e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2= LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(\n",
    "        template=template2,\n",
    "        input_variables=[\"summary\",\"fallacies_list\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8cc1a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSerperAPIWrapper(\n",
    "    type=\"news\",\n",
    "    tbs=\"qdr:m1\",  \n",
    "    serper_api_key=serper_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0d6c6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search topic\n",
    "search_topic = \"global trade\"\n",
    "\n",
    "# Get search results and load with WebBaseLoader and loader.load\n",
    "search_results = search.results(f\"site:whitehouse.gov {search_topic}\")\n",
    "article_url = search_results['news'][0]['link']\n",
    "article_title = search_results['news'][0]['title']\n",
    "loader = WebBaseLoader(article_url)\n",
    "article_text = ' '.join(loader.load()[0].page_content[:3000].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fcac11b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Running fact-checking analysis...\n",
      "‚úÖ Analysis complete!\n",
      "‚úÖ Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"üîç Running fact-checking analysis...\")\n",
    "summary = chain1.invoke({\"content\": article_text, \"fallacies_list\": fallacies_list})[\"text\"]\n",
    "analysis = chain2.invoke({\"summary\": summary, \"fallacies_list\": fallacies_list})[\"text\"]\n",
    "\n",
    "print(\"‚úÖ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ee36e391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing JSON extraction...\n"
     ]
    }
   ],
   "source": [
    "# Function to extract JSON from LLM response\n",
    "def extract_json_from_response(response_text):\n",
    "    \"\"\"Extract JSON from LLM response, handling various formats\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # First try: direct JSON parsing\n",
    "    try:\n",
    "        return json.loads(response_text.strip())\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    # Second try: find JSON between code blocks\n",
    "    json_match = re.search(r'```(?:json)?\\s*(\\{.*?\\})\\s*```', response_text, re.DOTALL)\n",
    "    if json_match:\n",
    "        try:\n",
    "            return json.loads(json_match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Third try: find JSON-like structure\n",
    "    json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "    if json_match:\n",
    "        try:\n",
    "            return json.loads(json_match.group(0))\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # If all fails, return structured error\n",
    "    return {\n",
    "        \"summary\": \"JSON parsing failed - raw response included below\",\n",
    "        \"claims\": [],\n",
    "        \"red_flags\": [\"Failed to parse LLM response as JSON\"],\n",
    "        \"confidence\": 0.0,\n",
    "        \"raw_response\": response_text\n",
    "    }\n",
    "\n",
    "# Test the JSON extraction\n",
    "print(\"üß™ Testing JSON extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ac6b983d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üì∞ FACT-CHECKING REPORT\n",
      "================================================================================\n",
      "üìÑ Article: Fact Sheet: President Donald J. Trump Further Modifies the Reciprocal Tariff Rates\n",
      "üîó URL: https://www.whitehouse.gov/fact-sheets/2025/07/fact-sheet-president-donald-j-trump-further-modifies-the-reciprocal-tariff-rates/\n",
      "================================================================================\n",
      "\n",
      "üìù SUMMARY:\n",
      "----------------------------------------\n",
      "['President Donald J. Trump signed an Executive Order modifying reciprocal tariff rates for certain countries to address US goods trade deficits.', \"The order reflects the President's efforts to protect the US against foreign threats to national security and economy.\", 'The President announced additional tariffs on April 2, and some countries have since agreed to trade deals and security agreements.', 'The modified tariff rates will apply to countries listed in Annex I of the Executive Order, while others will be subject to a 10% tariff.', \"The President aims to strengthen America's position in the global market by addressing decades of failed trade policy.\"]\n",
      "\n",
      "üéØ CONFIDENCE SCORE: 0.6\n",
      "\n",
      "üîç CLAIMS ANALYSIS:\n",
      "----------------------------------------\n",
      "\n",
      "1. CLAIM: President Trump signed an Executive Order modifying reciprocal tariff rates for certain countries.\n",
      "   VERDICT: True\n",
      "\n",
      "2. CLAIM: The President announced an additional 10% tariff on all countries on April 2.\n",
      "   VERDICT: True\n",
      "\n",
      "3. CLAIM: Several countries have agreed to, or are on the verge of agreeing to, meaningful trade deals and security agreements with the United States.\n",
      "   VERDICT: Needs context\n",
      "\n",
      "4. CLAIM: The President has reset decades of failed trade policy.\n",
      "   VERDICT: Misleading\n",
      "\n",
      "5. CLAIM: The modified tariff rates will strengthen America's position in the global market.\n",
      "   VERDICT: Unverifiable\n",
      "\n",
      "üö© RED FLAGS:\n",
      "----------------------------------------\n",
      "‚Ä¢ Sensational language used in the article, such as 'exploding, annual U.S. goods trade deficits' and 'take back America's economic sovereignty'.\n",
      "\n",
      "üéì ETHICS PROFESSOR REVIEW:\n",
      "----------------------------------------\n",
      "Here is my critique:\n",
      "\n",
      "1. Most impactful fallacy: Appeal to Emotion\n",
      "2. Why this could mislead readers: The article uses sensational language and emotional appeals to sway readers, rather than providing concrete evidence to support its claims. This can lead readers to make decisions based on emotions rather than facts.\n",
      "3. Counterfactual/counterpoint: The article may be using emotional appeals to compensate for a lack of concrete evidence or data to support its claims, or to distract from potential flaws in the President's trade policies.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def display_results(summary_json, ethics_analysis):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üì∞ FACT-CHECKING REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üìÑ Article: {article_title}\")\n",
    "    print(f\"üîó URL: {article_url}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Extract JSON using our robust function\n",
    "    data = extract_json_from_response(summary_json)\n",
    "    \n",
    "    print(\"\\nüìù SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(data.get('summary', 'No summary available'))\n",
    "    \n",
    "    print(f\"\\nüéØ CONFIDENCE SCORE: {data.get('confidence', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\nüîç CLAIMS ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    claims = data.get('claims', [])\n",
    "    for i, claim in enumerate(claims, 1):\n",
    "        print(f\"\\n{i}. CLAIM: {claim.get('claim', 'N/A')}\")\n",
    "        print(f\"   VERDICT: {claim.get('verdict', 'N/A')}\")\n",
    "        \n",
    "        fallacies = claim.get('fallacies', [])\n",
    "        if fallacies and fallacies != ['None found']:\n",
    "            print(f\"   FALLACIES: {', '.join(fallacies)}\")\n",
    "        \n",
    "        evidence = claim.get('evidence', [])\n",
    "        if evidence:\n",
    "            print(\"   EVIDENCE:\")\n",
    "            for j, ev in enumerate(evidence, 1):\n",
    "                print(f\"     {j}. \\\"{ev.get('quote', '')}\\\" - {ev.get('source', '')} ({ev.get('published_date', 'N/A')})\")\n",
    "        \n",
    "        notes = claim.get('notes', '')\n",
    "        if notes:\n",
    "            print(f\"   NOTES: {notes}\")\n",
    "    \n",
    "    red_flags = data.get('red_flags', [])\n",
    "    if red_flags:\n",
    "        print(\"\\nüö© RED FLAGS:\")\n",
    "        print(\"-\" * 40)\n",
    "        for flag in red_flags:\n",
    "            print(f\"‚Ä¢ {flag}\")\n",
    "    \n",
    "    # Show raw response if JSON parsing failed\n",
    "    if 'raw_response' in data:\n",
    "        print(\"\\n‚ö†Ô∏è  RAW LLM RESPONSE:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(data['raw_response'])\n",
    "    \n",
    "    print(\"\\nüéì ETHICS PROFESSOR REVIEW:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(ethics_analysis)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Display the results\n",
    "display_results(summary, analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5d05ad19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing JSON extraction...\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f3b82b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hetu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
